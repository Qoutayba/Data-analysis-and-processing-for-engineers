{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel methods\n",
    "\n",
    "by Felix Fritzen fritzen@mib.uni-stuttgart.de, January 2021\n",
    "\n",
    "additional material for the course *Data Processing for Engineers and Scientists*\n",
    "\n",
    "## Content\n",
    "- evaluation of the kernel matrix and checking if a kernel is PD (numerically)\n",
    "- kernel interpolation\n",
    "- kernel regression (least squares kernel regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import numpy.linalg as LA\n",
    "\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from scipy.special import binom\n",
    "\n",
    "from matplotlib import rc\n",
    "\n",
    "############################################################################\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['DejaVu Sans'], 'size':14})\n",
    "\n",
    "gridls      = { 'linewidth' : 3, 'color' : '#AAAAAA', 'linestyle' : ':' }\n",
    "legendstyle = dict(fontsize=16, facecolor='#DDDDDD', framealpha=1, \\\n",
    "                   loc='lower right', bbox_to_anchor = (0.975,0.025))\n",
    "f_size      = (12,3.75  )\n",
    "\n",
    "# define a few of the University's corporate design colors\n",
    "Sblue = '#004191'\n",
    "Slblue= '#00beff'\n",
    "Smagenta='#ec008d'\n",
    "Sgreen ='#8dc63f'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel matrix evaluation\n",
    "\n",
    "Given a kernel $k(\\underline{x},\\underline{y})$, $\\underline{x}, \\underline{y} \\in \\mathbb{R}^d$, and a set of points stored in the rows of a matrix $\\underline{\\underline{X}}\\in\\mathbb{R}^{n \\times d}$:\n",
    "- compute the kernel matrix in `kernel_matrix`\n",
    "- check numerically the PD properties `check_pd`\n",
    "The latter is done using random points (for simplicity the points are seeded in $\\varOmega = [-1, 1]^d$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_matrix( k, X, Z=None ):\n",
    "    \"\"\"\n",
    "    Evaluate the kernel matrix L_ij = k(x_i, z_j)\n",
    "    If Z is not provided, then Z=X is asserted\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    k:          symmetric kernel function (should accept d vectors of N x d matrices in the second argument)\n",
    "    X:          sample point x_i ( numpy.array( n, d ) )\n",
    "    Y:          sample point z_j ( None or numpy.array( m, d ) )\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    L:          kernel matrix ( numpy.array( n x m ))\n",
    "    \"\"\"\n",
    "    d = X.shape[-1]\n",
    "    m = int(X.size/d)\n",
    "    X = X.reshape((m,d))\n",
    "    \n",
    "        \n",
    "    if( Z is None ):\n",
    "        L = np.zeros( (m,m) )\n",
    "        for i in range(m):\n",
    "            L[i,:] = k( X[i,:], X )\n",
    "    else:\n",
    "        assert( d == Z.shape[-1] )\n",
    "        n = int(Z.size/d)\n",
    "        Z = Z.reshape((n,d))\n",
    "\n",
    "        L = np.zeros( (m,n))\n",
    "        for i in range(m):\n",
    "            L[i,:] = k( X[i,:], Z )\n",
    "    return L\n",
    "\n",
    "def check_pd( k, d, n_test = 1024, dim_test = 6 ):\n",
    "    \"\"\"\n",
    "    Test empirically whether the kernel is PD.\n",
    "    Therefore, each test comprises dim_test random points.\n",
    "    The test is repeated n_test times\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    k:          symmetric kernel function (should accept d vectors of N x d matrices in the second argument)\n",
    "    d:          dimension of the space ( integer )\n",
    "    n_test:     number of tests to perform ( integer )\n",
    "    dim_test:   number of points per test ( integer )\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    True/False depending on whether negative eigenvalues were obeserved\n",
    "    \"\"\"\n",
    "    TOL = -2.2e-16\n",
    "    for i in range(n_test):\n",
    "        K = kernel_matrix( k, np.random.uniform(-1, 1, size=(dim_test, d)))\n",
    "        eig = np.linalg.eigvalsh(K)\n",
    "        if( np.min(eig) < TOL ):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def gauss_kernel( gam, X, Y ):\n",
    "    \"\"\" Gauss kernel for all combinations of columns of X and Y\n",
    "    Parameters:\n",
    "    -----------\n",
    "    gam:         kernel parameter (>0)\n",
    "    X:           samples (numpy.array( m, d ))\n",
    "    Y:           samples (numpy.array( n, d ))\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    K            kernel evaluations K_ij = k(X[i,:], Y[j,:])\n",
    "    \"\"\"\n",
    "    d = X.shape[-1]\n",
    "    m = int(X.size/d)\n",
    "    X = X.reshape((m,d))\n",
    "    n = int(Y.size/d)\n",
    "    Y = Y.reshape((n,d))\n",
    "    \n",
    "    K = np.zeros( (m,n) )\n",
    "    for i in range(m):\n",
    "        x = X[i, :]\n",
    "        K[i,:] = np.exp( -gam* np.linalg.norm( x[None,:]-Y, axis=1)**2 )\n",
    "    return K    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d       = 8\n",
    "dim_test= 12\n",
    "n_test  = 1024\n",
    "gam     = 2.0\n",
    "k_gauss = lambda x,y: gauss_kernel( gam, x, y)\n",
    "\n",
    "X = np.random.uniform(-1,1,size=(dim_test,8))\n",
    "Y = np.random.uniform(-1,1,size=(8))\n",
    "\n",
    "print('Kernel matrix is pd (%d checks, each with %d points; dim=%d): ' % (n_test, dim_test, d), \\\n",
    "      check_pd( k_gauss, 8, n_test=n_test, dim_test=dim_test ))\n",
    "\n",
    "# condition of the kernel matrix for different values of gamma\n",
    "n = 8\n",
    "for gam in 10**np.linspace(-4, 1, 10):\n",
    "    k_gauss = lambda x,y: gauss_kernel( gam, x, y)\n",
    "    eig = np.linalg.eigvalsh( kernel_matrix( k_gauss, np.random.uniform(-1,1,size=(n,d))) )\n",
    "    print('condition of the kernel matrix (gamma: %8.4f):   %14.4e' %(gam, eig.max()/eig.min()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel interpolation\n",
    "\n",
    "The noisy function\n",
    "$$f(x) = \\sin(x) + g, \\qquad g \\sim \\mathcal{N}(0,\\sigma^2), \\qquad \\sigma>0$$\n",
    "is investigated for $x \\in \\varOmega =  [-\\pi, \\pi]$. Two different choices are made for the interpolation points:\n",
    "- uniformly random points $\\varOmega$\n",
    "- regular points on $\\varOmega$\n",
    "\n",
    "The Gauss kernel is used with varying kernel parameter $\\gamma>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_kernel_int( x, y, gam, delta=0.0 ):\n",
    "    \"\"\" Setup the kernel interpolation:\n",
    "    given x and y data and the kernel parameter gamma\n",
    "    as well as an optional nugget parameter delta>=0,\n",
    "    the method returns a list that can directly be used by\n",
    "    apply_kernel_int\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x, y:       x and y data ( numpy.array(n) )\n",
    "    gam:        kernel parameter > 0  for the Gauss kernel (float)\n",
    "    delta:      nugget parameter delta >= 0 (float)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    k_int_data: a dictionary containing the weights, x, y and the metadata (gamma and delta)\n",
    "    \"\"\"\n",
    "    from scipy.linalg import cho_factor, cho_solve\n",
    "    assert(x.ndim==2)\n",
    "    assert(x.shape[0]==y.shape[0])\n",
    "    k_gauss = lambda x, y: gauss_kernel( gam, x, y )\n",
    "    K = kernel_matrix( k_gauss, x )\n",
    "    if( delta > 0):\n",
    "        K=K+np.diag(delta*np.ones(x.shape[0]))\n",
    "    c, low = cho_factor( K )\n",
    "    w = cho_solve( (c, low), y )\n",
    "    return dict( x=x, y=y, w=w, gam=gam, delta=delta )\n",
    "\n",
    "def apply_kernel_int( x, k_int_data ):\n",
    "    \"\"\" Apply the kernel interpolation along the rows of x\n",
    "    (call setup_kernel_int beforehand).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x:          np.array( n, d )\n",
    "    k_int_data: list from setup_kernel_int\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y:          np.array(n)\n",
    "    \"\"\"\n",
    "    k = gauss_kernel( k_int_data['gam'], x, k_int_data['x'])\n",
    "    return ( k @ k_int_data['w'] )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 12\n",
    "n_ref = 1024\n",
    "\n",
    "X = []\n",
    "X.append( np.random.uniform( -np.pi, np.pi, size=(n,1) ) )\n",
    "X.append( np.linspace(-np.pi, np.pi, n).reshape(n, 1 ) )\n",
    "\n",
    "sigma   = 1e-2\n",
    "f       = lambda x: np.sin( x ) + np.random.normal( 0, sigma, size=x.shape )\n",
    "x_ref   = np.linspace( -np.pi, np.pi, n_ref  )\n",
    "y_ref   = np.sin(x_ref)\n",
    "x_ref   = x_ref.reshape((x_ref.size, 1))\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize=(18,12))\n",
    "ax      = ax.flatten()\n",
    "ax_it   = iter(ax)\n",
    "Y       = []\n",
    "ERR     = []\n",
    "ct      = 0\n",
    "delta   = 1e-15\n",
    "text    = [ 'random', 'equidistant' ]\n",
    "text_it = iter(text)\n",
    "for x in X:\n",
    "    txt = next(text_it)\n",
    "    y   = f(x).reshape(x.size,1).flatten()\n",
    "    Y.append(y)\n",
    "    for gam in np.array([0.1,0.5,1]):\n",
    "        k_int_data = setup_kernel_int( x, y, gam, delta=delta)\n",
    "        y_int = apply_kernel_int( x_ref, k_int_data )\n",
    "        ERR.append( y_ref - y_int )\n",
    "        AX= next(ax_it)\n",
    "        AX.scatter( x.flatten(), y, color='black' )\n",
    "        AX.plot( x_ref, ERR[-1], color='red', linewidth=2, label=r'error' )\n",
    "        AX.plot( x_ref, y_ref, color= Sblue, linewidth=2, label='reference')\n",
    "        AX.plot( x_ref, y_int, dashes = [2, 2], color= Slblue, linewidth=2, label='interpolation' )\n",
    "        AX.set_title('interp. err. $\\gamma=%.2f$, %s points ' %(gam,txt) )\n",
    "        AX.legend()\n",
    "        AX.grid()\n",
    "        AX.set_ylim((-1.2,1.2))\n",
    "        print('l^2 error - gam = %6.2f -  %16.8e   (%s)' \\\n",
    "              %(gam, np.linalg.norm(ERR[-1].flatten())/np.sqrt(n_ref), txt ) )\n",
    "        ct+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k_int_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- regular point seeds outperform the random points\n",
    "- the influence of noisy data is more pronounced for random points\n",
    "- the interpolation can get oscillatory\n",
    "- poor conditioning of the kernel matrix can happen for random points and/or small $\\gamma$\n",
    "- race condition: small $\\gamma$ is optimal for noise free, smooth data; with increasing $\\sigma$, increasing $\\gamma$ gives better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Kernel regression\n",
    "\n",
    "The same function with the same noise is considered. Only 6 support points are selected but 12 (random) data samples are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_kernel_reg( z, x, y, gam ):\n",
    "    \"\"\" Setup the kernel regression:\n",
    "    given x and y data and the kernel parameter gamma\n",
    "    the method returns a dictionary that can directly be used by\n",
    "    apply_kernel_reg\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z:          numpy.array(n_sup,d)\n",
    "                support points\n",
    "    x:          numpy.array(n,d)\n",
    "                sample point coordinates\n",
    "    y:          numpy.array(n)\n",
    "                function data y \n",
    "    gam:        kernel parameter > 0  for the Gauss kernel (float)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    k_reg_data: a dictionary containing the weights, z, x, y, gamma and y_sup\n",
    "    (the virtual value at the support points)\n",
    "    \"\"\"\n",
    "    from scipy.linalg import cho_factor, cho_solve\n",
    "    assert(x.ndim==2)\n",
    "    assert(x.shape[0]==y.shape[0])\n",
    "    assert(z.shape[1]==x.shape[1])\n",
    "    n_sup = z.shape[0]\n",
    "    n = x.shape[0]\n",
    "    d = z.shape[1]\n",
    "    L = gauss_kernel( gam, x, z )\n",
    "    V, sig, W_t = np.linalg.svd( L, full_matrices=False  )\n",
    "    sig_i = 1/sig\n",
    "    sig_i[sig < 1e-15] = 0\n",
    "    w   = W_t.T @ ( np.diag(sig_i) @ ( V.T@y ))\n",
    "    y_sup = gauss_kernel( gam, z, z) @ w\n",
    "    return dict( z=z, x=x, y=y, w=w, gam=gam, y_sup=y_sup )\n",
    "\n",
    "def apply_kernel_reg( x, k_reg_data ):\n",
    "    \"\"\" Apply the kernel regression along the rows of x\n",
    "    (call setup_kernel_reg beforehand).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x:          np.array( n, d )\n",
    "    k_int_data: list from setup_kernel_int\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y:          np.array(n)\n",
    "    \"\"\"\n",
    "    k = gauss_kernel( k_reg_data['gam'], x, k_reg_data['z'])\n",
    "    return ( k @ k_reg_data['w'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sup = 4\n",
    "\n",
    "x = X[0] # np.random.uniform( -np.pi, np.pi, size=(n_sample,1) )\n",
    "z = np.linspace(-np.pi, np.pi, n_sup).reshape(n_sup, 1 )\n",
    "\n",
    "fig, ax = plt.subplots(2,3, figsize=(18,12))\n",
    "ax      = ax.flatten()\n",
    "ax_it   = iter(ax)\n",
    "ERR     = []\n",
    "ct      = 0\n",
    "Y_it    = iter(Y)\n",
    "\n",
    "K   = kernel_matrix( k_gauss, x )\n",
    "ct_x =0\n",
    "text_it = iter(text)\n",
    "for x in X:\n",
    "    y = next(Y_it)\n",
    "    txt = next(text_it)\n",
    "    for gam in np.array([0.1,0.5,1.00]):\n",
    "        k_reg_data = setup_kernel_reg( z, x, y, gam=gam )\n",
    "        y_int = apply_kernel_reg( x_ref, k_reg_data )\n",
    "        ERR.append( y_ref - y_int )\n",
    "        y_Z = k_reg_data['y_sup'].copy()\n",
    "        AX  = next(ax_it)\n",
    "        AX.scatter( z.flatten(), y_Z.flatten(), color='green', label='support points')\n",
    "        AX.scatter( x.flatten(), y, color='black', label='samples', alpha=0.4, s=6 )\n",
    "        AX.plot( x_ref, ERR[-1], color='red', linewidth=2, label=r'error' )\n",
    "        AX.plot( x_ref, y_ref, color= Sblue, linewidth=2, label='reference')\n",
    "        AX.plot( x_ref, y_int, dashes = [2, 2], color= Slblue, linewidth=2, label='regression' )\n",
    "        AX.set_title('interp. err. $\\gamma=%.2f$ - %s' %(gam,txt) )\n",
    "        AX.legend()\n",
    "        AX.grid()\n",
    "        AX.set_ylim((-1.2,1.2))\n",
    "        print('l^2 error - gam = %6.2f -  %16.8e   (regression, n_sup = %d, %s)' \\\n",
    "              %(gam, np.linalg.norm(ERR[-1].flatten())/np.sqrt(n_ref) , n_sup, txt) )\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the influence of the number of samples\n",
    "\n",
    "- keep number of support points constant (e.g. at 12)\n",
    "- increase the number of sample points\n",
    "- use random or uniformly seeded samples\n",
    "- investigate the error for different realizations\n",
    "- investagate the influence of $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the kernel regression with more samples\n",
    "n_sample = 36\n",
    "n_sup = 12\n",
    "gam = 1.\n",
    "# random samples:\n",
    "x = np.random.uniform(-np.pi, np.pi, size=(n_sample, 1))\n",
    "# uniform samples:\n",
    "# x = np.linspace(-np.pi, np.pi, n_sample).reshape(n_sample,1)\n",
    "x_sup = np.linspace( -np.pi, np.pi, n_sup ).reshape(n_sup,1)\n",
    "k_reg_data = setup_kernel_reg( x_sup, x, f(x), gam = gam )\n",
    "y_int = apply_kernel_reg( x_ref, k_reg_data ).flatten()\n",
    "err = y_ref-y_int\n",
    "print('l^2 error - gam = %6.2f -  %16.8e   (regression, n_sup = %d, random %d)' \\\n",
    "      %(gam, np.linalg.norm(err.flatten())/np.sqrt(n_ref) , n_sup, n_sample) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- small influence of $\\gamma$\n",
    "- error decays approximately with $1/n_\\mathsf{s}$, i.e. more samples help ruling out the influence of the noise\n",
    "- errors are smaller than for the equidistant interpolation with optimal $\\gamma$ (particularly for higher noise $\\sigma$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
#####