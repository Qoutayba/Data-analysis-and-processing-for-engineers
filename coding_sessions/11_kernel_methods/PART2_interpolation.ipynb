{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel methods\n",
    "\n",
    "Author: Julian Li√üner\n",
    "\n",
    "For questions and feedback write a mail to: [lissner@mechbau.uni-stuttgart.de](mailto:lissner@mechbau.uni-stuttgart.de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.extend( ['provided_functions', 'incomplete_functions' ])\n",
    "import kernels\n",
    "import model_evaluation as evaluate\n",
    "\n",
    "import result_check as check\n",
    "import display_sets as display\n",
    "import sample_sets as sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Interpolation and regression\n",
    "- they are closely related but have a distinct difference\n",
    "- in interpolation each support point (training sample) is matched exactly\n",
    "- in regression the support points are chosen and training samples are fit under the given loss\n",
    "\n",
    "### Interpolation\n",
    "\n",
    "- given some support points (training samples), interpolating these will yield an approximation of the original function for good hyperparameters\n",
    "- the coefficients $\\underline w$ (model weight) are given as $\\underline w = (\\underline{\\underline K}(\\gamma))^{-1} \\, \\underline y \\quad$ <br>with $\\, \\underline{\\underline K}(\\gamma) \\hat = k( x_i, x_j; \\gamma) $ for all training samples in $x$ and $y$\n",
    "- the model can then be evaluated as $ \\underline{\\hat y}(\\underline x_{\\rm valid} ) = \\underline{w} \\, \\underline{\\underline{K}} ( \\underline{x}, \\underline x_{\\rm valid}; \\gamma )$\n",
    "\n",
    "-----\n",
    "__Task:__ Compute the kernel matrix using the available training data. Apply it for the interpolation of `x_valid`.<br>\n",
    "Note that multiple executions will give you different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# all variables below can be adjusted\n",
    "my_function = lambda x: -0.2*x**2 + np.sin( 2*x) + 0.5*(x+1)\n",
    "interval = [0, 4]\n",
    "n_train = 6\n",
    "x_train, y_train = sample.function( my_function, interval, n_train) \n",
    "x_valid = sample.uniform_interval( *interval)\n",
    "\n",
    "\n",
    "slope = 0.5\n",
    "nugget = 1e-12\n",
    "\n",
    "kernel_matrix = kernels.linear( x_train, x_train, slope)\n",
    "kernel_matrix -= nugget* #TODO #subtract the nugget on the diagonal\n",
    "K_inv = #TODO\n",
    "coefficients = #TODO\n",
    "kernel_response = kernels.linear( x_train, x_valid, slope)\n",
    "y_interpolated = #TODO #evaluation of x_valid using 'kernel_response'\n",
    "\n",
    "fig, ax = plt.subplots( figsize=(6,6) )\n",
    "ax.scatter( x_train, y_train, facecolor='lightblue', edgecolor='k', label='support points' )\n",
    "ax.plot( x_valid, my_function( x_valid), color='blue', linewidth=2, label='reference solution' )\n",
    "ax.plot( x_valid, y_interpolated, color='red', linewidth=2, label='interpolation' )\n",
    "ax.grid( ls=':' )\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "- since the linear kernel is rather bad, different kernels can improve the result\n",
    "- the gaussian kernel is a generally applicable kernel for a wide range of problems\n",
    "- the gaussian kernel is defined as $k(x, x^{\\prime}) = \\exp( -\\gamma ||x-x^{\\prime}||_2^2 ) $\n",
    "- an efficient implementation of the gaussian kernel for higher dimensional samples is already provided\n",
    "----\n",
    "__Task:__ Implement the `gaussian_1d` kernel in 'kernels.py'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "check.kernel_implementation( kernels.gaussian_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "- hyperparameters are parameters the model can not learn, the expert (i.e. you) has to adjust them\n",
    "- here, hyperparameters are:\n",
    "    - kernel function\n",
    "    - parameters of the function\n",
    "- the MSE (mean squared error) is a good error measure for model evaluation, it is defined as\n",
    "$$ MSE = \\frac1n\\sum\\limits_{i=1}^{n} (y_i -\\hat y_i)^2 $$\n",
    "\n",
    "----\n",
    "__Task:__ Write the function `interpolation` in 'model_evaluation.py' and tune the hyperparameters to get a good match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optional, adjust the sampled function\n",
    "my_function = lambda x: -0.2*x**2 + np.sin( 2*x) + 0.5*(x+1)\n",
    "interval = [0, 4]\n",
    "n_train = 6\n",
    "x_train, y_train = sample.function( my_function, interval, n_train, noise=0) \n",
    "x_valid = sample.uniform_interval( *interval)\n",
    "y_valid = my_function( x_valid)\n",
    "\n",
    "## Hyperparameters\n",
    "## if you change the kernel function you will need different parameters\n",
    "#help( kernels)\n",
    "kernel_function = kernels.gaussian_1d #TODO #try different kernels\n",
    "parameters = [ 1.01] \n",
    "\n",
    "## Interpolation\n",
    "y_interpolated = evaluate.interpolation( #TODO, kernel_function, parameters, y_train=y_train, nugget=#TODO ) \n",
    "MSE = lambda y_1, y_2: #TODO\n",
    "print( 'achieved MSE:', MSE( y_valid, y_interpolated) )\n",
    "\n",
    "## Plotting\n",
    "fig, ax = plt.subplots( figsize=(6,6) )\n",
    "ax.scatter( x_train, y_train, facecolor='lightblue', edgecolor='k', label='support points' )\n",
    "ax.plot( x_valid, y_valid, color='blue', linewidth=2, label='reference solution' )\n",
    "ax.plot( x_valid, y_interpolated, color='red', linewidth=2, label='interpolation' )\n",
    "ax.grid( ls=':' )\n",
    "_=ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Least square regression\n",
    "- in regression the best fit for multiple samples is searched\n",
    "- interpolating these samples would usually lead to _overfitting_/oscillations\n",
    "- the evaluation for regression is the same as for interpolation\n",
    "- the model training is different with manually selected support points $\\underline z$ under consideration of the loss/cost function\n",
    "- the formula for the `coefficients` $\\underline w$ is given as\n",
    "$$ \n",
    "\\underline w = (\\underline{\\underline L}^T\\,\\underline{\\underline L})^{-1}\\,\\underline{\\underline L}^T \\, \\underline y_{\\rm train} $$\n",
    "\n",
    "$\\quad$ with $\\underline{\\underline L} = k( x_i, z_j;\\gamma)$ for all training samples $x_i$ and support points $z_j$\n",
    "- the model can then be evaluated as $ \\underline{\\hat y}(\\underline x^\\prime ) = \\underline{w} \\, \\underline{\\underline{K}} ( \\underline{x}, \\underline x^\\prime; \\gamma )$\n",
    "- note that unlike $\\underline{\\underline K}$, the kernel matrix $\\underline{\\underline L}$ is generally not rectangular \n",
    "- the computation of ${\\underline w}$ can be implemented more efficiently using the SVD\n",
    "- recall the definition $\\text{SVD}( \\underline{\\underline L}) = \\underline{\\underline V}\\, \\underline{\\underline{\\sigma}}\\, \\underline{\\underline W}^T$ with $\\underline{\\underline V}^T \\underline{\\underline V}=\\underline{\\underline I}$ and $\\underline{\\underline W}^T \\underline{\\underline W}=\\underline{\\underline I}$\n",
    "------\n",
    "__Task:__ Write out the reformulation of the equation for ${\\underline w}$ using the SVD and derive the efficient implementation on paper. Implement the efficient computation of the weights.<br>\n",
    "Tune the hyperparameters to get a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "## Definition of data, Optional: adjust the sampled function\n",
    "my_function = lambda x: (-2*np.tanh( x-5) -2 + 1/3*x**2 -1/20*x**3 + np.exp( x/3) -1e-4*np.exp(x) -2*np.abs(x)**0.3).flatten()\n",
    "interval = [0, 10]\n",
    "n_samples = 125\n",
    "noise = 0.15\n",
    "\n",
    "x_train, y_train = sample.function( my_function, interval, n_samples, noise) \n",
    "x_valid = sample.uniform_interval( *interval)\n",
    "y_valid = my_function( x_valid)\n",
    "\n",
    "\n",
    "## Hyperparameters\n",
    "kernel_function = kernels.linear #TODO #choose different kernel functions\n",
    "parameters = [ 0.52] #TODO\n",
    "support_points = np.array( [9, 1, 1.2] ) #TODO\n",
    "\n",
    "\n",
    "## Model training\n",
    "kernel_matrix = kernel_function( #TODO..., *parameters) \n",
    "L = kernel_matrix\n",
    "coefficients = #TODO\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "y_regression = evaluate.interpolation( #TODO, x_valid, kernel_function, parameters, coefficients) \n",
    "\n",
    "print( 'achieved MSE:', MSE( y_regression,y_valid) ) \n",
    "## Plotting\n",
    "y_support = evaluate.interpolation( support_points, support_points, kernel_function, parameters, coefficients) \n",
    "fig, ax = plt.subplots( figsize=(6,6) )\n",
    "ax.scatter( x_train, y_train, facecolor='lightblue', edgecolor='k', alpha=0.5, label='training samples' )\n",
    "ax.scatter( support_points, y_support, facecolor='red', edgecolor='k', label='support points' )\n",
    "ax.plot( x_valid, y_valid, color='blue', linewidth=2, label='reference solution' )\n",
    "ax.plot( x_valid, y_regression, color='red', linewidth=2, label='interpolation' ) \n",
    "\n",
    "ax.grid( ls=':' )\n",
    "_=ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
